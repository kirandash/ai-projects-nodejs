## Build AI Projects with Node.js, OpenAI, LangChain, Vector DBs, HuggingFace and TypeScript

## 1. OpenAI Fundamentals

### 1.1 OpenAI Setup, Create api key and run first prompt

- docs: https://platform.openai.com/docs/quickstart
- OpenAI API Key
  - https://openai.com/index/openai-api/
  - https://platform.openai.com/
- Create a node/npm project
  - `npm init -y`
  - `npm install openai`
- run a prompt
- Note: `--env-file=.env` is used to load the environment variables from the .env file

### 1.2 Typescript setup

- `npm i -D typescript ts-node @types/node`
- `npx tsc --init`

### 1.3 OpenAI and Open Source Models ex HuggingFace

- What is a model?
  - A model is a machine learning system that generates predictions based on data. It is the core of the OpenAI API.
  - The OpenAI API provides a range of models that can be used to generate text, images, and more.
- Large language models are the most popular type of model. They can generate human-like text based on a prompt.
- OpenAI Models
  - https://platform.openai.com/docs/models
  - GPT-4o
    - CONTEXT WINDOW: A context window is a window of text that the model uses to generate predictions. The context window size is the number of tokens in the window.
    - training data date: date till which the model was trained
    - price: cost per token
    - https://platform.openai.com/docs/models/gpt-4o
  - GPT-4 turbo
  - GPT-3.5 Turbo
  - DALLÂ·E
  - Whisper
- open source models
  - HuggingFace: https://huggingface.co/
  - https://huggingface.co/models

### 1.5 Tokens:

- https://platform.openai.com/tokenizer
- Tokens are the smallest unit of text that the model can generate. The model generates text by predicting the next token in a sequence of tokens.
- Tokens can be words, punctuation, or other characters.
- The model generates tokens based on the context window, which is a window of text that the model uses to generate predictions.
- The context window size is the number of tokens in the window.
- Learn more about tokenization: https://huggingface.co/learn/nlp-course/en/chapter6/1?fw=pt
- the pricing depends on tokens:
  - https://openai.com/api/pricing/
- see usage at: https://platform.openai.com/usage
- how to check tokens in playground:
  you can go to the playground at: https://platform.openai.com/playground/chat?models=gpt-4o
- to check tokens in nodejs project:
  - https://github.com/dqbd/tiktoken
  - `npm i tiktoken`

### 1.7 Roles

- System
- User
- Assistant

### 1.8 OpenAI API and playground parameters

- Temperature
  - The temperature parameter controls the randomness of the model's predictions. A higher temperature will result in more random predictions, while a lower temperature will result in more predictable predictions.
  - recommended range: 0.1 to 1.0
- Top P:
  - The top_p parameter controls the diversity of the model's predictions. A higher top_p will result in more diverse predictions, while a lower top_p will result in more focused predictions.
  - ex: the top_p parameter is set to 0.9, the model will only consider the top 90% of tokens with the highest probabilities when generating predictions.
  - recommended range: 0.1 to 0.9
- Maximum tokens
  - The max_tokens parameter controls the maximum number of tokens that the model will generate in a single response. If the model reaches the maximum number of tokens, it will stop generating text.
  - recommended range: 16 to 2048
- Frequency penalty:
  - The frequency_penalty parameter controls the model's tendency to repeat tokens that have already been generated. A higher frequency_penalty will result in less repetition, while a lower frequency_penalty will result in more repetition.
  - recommended range: 0.1 to 2.0
- n:
  - number of choices: the number of completions to generate. default: 1
- seed: 88888
  - If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.

## 2 Simple Chat Project

- Features:
  - Chat with OpenAI
  - Add chat history (context)
  - Set token limit

### 2.1 Setup new chat-app project

- `npm init -y`
- `npm i -D typescript ts-node @types/node`
- `npx tsc --init`
- `npm i openai tiktoken`

### 2.2 Build the Chat with OpenAI

- we will use the console as the chat interface
  - use `process.stdin.addListener`

### 2.3 Add chat history (context)

- you can build this by simply pushing the user and assistant messages to an array and then using that array as the context for the next message

### 2.4 Set token limit

- Why keep context small?
  - to control the cost
  - to avoid any errors due to large context since gpt-3.5-turbo has a limit of 16,385

### 3 OpenAI Function calling or Tools

- https://platform.openai.com/docs/guides/function-calling
- Why function calling?
  - Chatgpt only has access to the trained data set ex: for gpt 3 the training data is till 2021
  - to get the latest data we can use function calling
    - ex: what is the latest price of bitcoin or what is the date today?
    - to solve this we can use function calling. So the openai model can be prompted to call a function to get the latest data
- so we can do
  - access the latest data or real time data
  - modify/mutation of data ex: in a DB
  - act like a live chat bot which can interact with the user in real time

Topics to cover:

- setup function calling
- parameters of the function
- multiple functions
- project for demo

### 4. Setup function calling

- Configure the function calling
- Decide which function to call
- Call the function
- call openAI with the function response
